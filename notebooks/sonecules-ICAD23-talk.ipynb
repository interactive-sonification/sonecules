{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785b42e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dbb988",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sonecules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c81c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run prepare-data.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03dcf50",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### ICAD 2023\n",
    "# sonecules: A Python Sonfication Architecture\n",
    "### Dennis Reinsch, Thomas Hermann\n",
    "#### Ambient Intelligence Group, Bielefeld University"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d12dd4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sonification Process\n",
    "\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"./figures/ecosystem-transpose-process.drawio.svg\" width=100%/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a29f1be",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Aproaches to Sonification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabe6708",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* two extremes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7389186a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* (i) open sound synthesis platforms high-jacked for sonification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf27e0d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* (ii) all-in-one graphical sonification design programs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16e7d03",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* both create a walled garden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e697cb4",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "two extrema\n",
    "(i) open sound synthesis platforms high-jacked for sonification\n",
    "- examples: SuperCollider, PureData, Max/MSP, Csound\n",
    "- requires knowledge in regard of sound synthesis to even get started.\n",
    "  something what seems strange when we relate it to visualization\n",
    "- often lack data handling\n",
    "(ii) complex all-in-one graphical sonifi- cation design programs: in this case complete/self-contained pack- ages are provided which often come with GUI, data import func- tions, and one (or few) very specific sonification design(s).\n",
    "- examples: Highcharts Sonification Studio, Sonification Workstation, or Rotator\n",
    "- Such systems are much more beginner-friendly as compared to the above class of approaches,\n",
    "- large code base to integrate the implemented sonification design with specific data loading and processing capabilities and in addition provide a graphical user interface for all this.\n",
    "- but they are quite limited to their specific use case as they are hard (if not impossible) to extend â€“ or even to be adjusted in parameters, if developers did not already offer a control for it.\n",
    "\n",
    "\n",
    "To our opinion, both of these extremes create some kind of walled garden which hinders the sonification community to share their methods and grow\n",
    "We believe that the walls can be teared down by following the example of how computer graphics was made available to users,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd549d01",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Toolchain Solution\n",
    "\n",
    "<center>\n",
    "<img src=\"./figures/ecosystem-transpose.drawio.svg\" width=100%/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a8c9db",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* do one thing but do it well - UNIX principle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add50721",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fundamental Concepts\n",
    "\n",
    "* Context and Timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d05d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sonecules.startup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ad8356",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = sonecules.gcc()\n",
    "\n",
    "s1 = context.synths.create(\"s1\")\n",
    "\n",
    "with context.at(1):\n",
    "    s1.start(freq=300)\n",
    "    \n",
    "context.timeline.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d4a850",
   "metadata": {},
   "outputs": [],
   "source": [
    "playback = sn.playback()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50e8723",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sonecules Demo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6890bd2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f9184c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Critical Goals\n",
    "\n",
    "  a) flexibility and portability\n",
    "  \n",
    "  b) availability and distribution\n",
    "  \n",
    "  c) reproducibility\n",
    "  \n",
    "  d) benchmarking, enchancing scientific standards and development\n",
    "  \n",
    "  e) dissemination\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce2c5a0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "\n",
    "* design-once-use-many approach "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3afa0b8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbca032b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sonification Pipeline - All in One\n",
    "\n",
    "<center>\n",
    "<img src=\"./figures/soni-pipeline-allinone-small.jpg\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb931af7",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "IN: The first example are tools that offer the user to handle the complete pipeline\n",
    "\n",
    "However the resulting tools are often very specialized towards certain tasks and often can be regarded  as a graphical user interface for a single sonification design.\n",
    "\n",
    "This does not allow the user to create completely new sonifications as the tools are not flexible enough, but rather use a certain sonification without much effort.\n",
    "\n",
    "\n",
    "OUT: This is why many sonification experts prefer to built their own custom tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98f8e64",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sonification Pipeline - Combination of Tools\n",
    "\n",
    "<center>\n",
    "<img src=\"./figures/soni-pipeline-combi-small.jpg\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6c7f02",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* sc3nb = Python + SuperCollder in Jupyter notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697bcac7",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "IN: A common approach here is to use a sound synthesis engine like SuperCollider for creating the sound \n",
    "\n",
    "the data is prepared beforehand with other tools like Python.\n",
    "\n",
    "\n",
    "While this approach offers the user a more flexible way it still requires the user to have knowledge about used sound synthesis engine\n",
    "\n",
    "\n",
    "additionally it often becomes complicated to mix and match the different tools with each other and share the data\n",
    "\n",
    "\n",
    "\n",
    "To tackle this problem we already created sc3nb which makes SuperCollider accessible from Python and allows the combined usage in one interactive Jupyter notebook\n",
    "\n",
    "However sc3nb is strictly tied to SuperCollider and requires that the user is familiar with SuperCollider \n",
    "\n",
    "\n",
    "OUT: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff67cce",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# mesonic\n",
    "\n",
    "<center>\n",
    "<img src=\"./figures/mesonic-pipeline.jpg\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f90ca79",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "IN: mesonic wants to go further by more directly focusing on the transformation.\n",
    "\n",
    "this is done by providing a meso level that should be more flexible than high level (all in one) approaches \n",
    "\n",
    "but it should still offer the user an a shortcut to create new sonifications designs\n",
    "\n",
    "instead of requiring the creation of custom tools, which are often hard to share and maintain\n",
    "\n",
    "and require the user to deeply dive down into sound synthesis engines.\n",
    "\n",
    "While at least some knowledge about sound synthesis is required to create a sonification it should be noted that the\n",
    "most sound synthesis tools are quite complex and can be intimidating for new users that want create a new sonification\n",
    "\n",
    "Additionally it is important to note that most sound synthesis engines are not specifically crafted for sonifications and often lack data processing capabilities.\n",
    "\n",
    "OUT: The idea to introduce a layer in between the low level sound synthesis world and the high level sonification applications is inspired by the visualization domain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c5f6be",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# matplotlib\n",
    "\n",
    "<center>\n",
    "<img src=\"./figures/anatomy-of-figure.jpg\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b9c360",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "IN: If we look at matplotlib f.e. it becomes obvious that .. \n",
    "\n",
    "the plots does consists of different objects\n",
    "\n",
    "These objects together form the complete plot \n",
    "\n",
    "As a user I do not need to create every single linie\n",
    "but i can create complex plots by single function calls that will provide me all the single objects\n",
    "\n",
    "Additionally it is possible to fine tune the plot using a object oriented approach.\n",
    "and adjust f.e. the limits and the title.\n",
    "\n",
    "This is what makes matplotlib easy to use but still a flexible basis for many other applications.\n",
    "\n",
    "OUT: mesonic tries to adapt this idea to form a sonification from single parts and other ideas from the visualization domain into the domain of sonification\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3192edf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# mesonic - audio objects\n",
    "\n",
    "<center>\n",
    "<img src=\"./figures/mesonic-synth.jpg\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3531b0c5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* practical use: discrete vs continuous Parameter Mapping Sonification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2e9838",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* inspired by Enge et al. 2021 - **0D / 1D auditory mark**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057cfbcd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# mesonic - audio objects\n",
    "\n",
    "<center>\n",
    "<img src=\"./figures/mesonic-buffer-record.jpg\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5a41fb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffaccec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mesonic\n",
    "import numpy as np\n",
    "from pya import *\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from sc3nb import linlin, midicps, cpsmidi\n",
    "import sc3nb as scn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dc540e",
   "metadata": {},
   "source": [
    "Lets start by preparing mesonic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e2bd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = mesonic.create_context()\n",
    "context.enable_realtime();\n",
    "context.processor.latency = 0.05\n",
    "context.processor.latency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b90ab6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In this example we will use the EEG data from the [Supplementary material for \"sc3nb: a Python-SuperCollider Interface for Auditory Data Science\"](https://doi.org/10.4119/unibi/2956379)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f50fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt(\"./files/epileptic-eeg.csv\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6726788c",
   "metadata": {},
   "source": [
    "We can simply create a stereo Buffer using this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5871a45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "buf = context.buffers.from_data(data[:,[0,1]], sr=256)\n",
    "buf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7037344",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "And create a default Synth to play it back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd65006",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsyn = context.synths.from_buffer(buf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5238efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsyn # to see the synth's controls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dea05b1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "let's audify the data in a loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2ff942",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsyn.start(rate=20, amp=0.1, loop=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe18e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsyn.rate = 5\n",
    "bsyn.amp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8991b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsyn.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daab587",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A more advanced example using Granular Synthesis for interactive scrubbing of the buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b80235e",
   "metadata": {},
   "outputs": [],
   "source": [
    "context.synths.buffer_synthdefs[\"tgrains\"]= r\"\"\"\n",
    "{ | bufnum={{BUFNUM}}, amp=0.3, rate=10, trate=5, pos=0 |\n",
    "    var dur, cpos, sig;\n",
    "    dur = 4 / trate;\n",
    "    cpos = pos * BufDur.kr(bufnum);\n",
    "    sig = TGrains.ar(2, Impulse.ar(trate), bufnum, rate, cpos, dur, 0, 0.5, 2);\n",
    "    Out.ar(0, sig * amp);\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7261c7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "buf = context.buffers.from_data(data[:,5], sr=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033c686a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tgsyn = context.synths.from_buffer(buf, synth_name=\"tgrains\")\n",
    "tgsyn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc0c5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tgsyn.start(rate=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a422ea14",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "fig, ax = plt.subplots(figsize=(8,2))\n",
    "asig = Asig(data[:,5], sr=256).plot()\n",
    "\n",
    "def on_move(event):\n",
    "    if event.inaxes and event.button is mpl.backend_bases.MouseButton.LEFT:\n",
    "        tgsyn.rate =  20 if event.ydata > 0 else 50\n",
    "        tgsyn.pos = linlin(event.xdata, 0, 50, 0, 1)\n",
    "\n",
    "binding_id = fig.canvas.mpl_connect('motion_notify_event', on_move)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b8a400",
   "metadata": {},
   "outputs": [],
   "source": [
    "tgsyn.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a877c2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# mesonic concepts\n",
    "\n",
    "<center>\n",
    "<img src=\"./figures/mesonic-concepts.jpg\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99d0ccc",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Here you can see an overview of the concepts used in mesonic\n",
    "\n",
    "- the Context can be regarded as counterpart of the Figure from matplotlib and as the central interface\n",
    "    - it controls the backend which is clearly separated and designed to be exchangeable\n",
    "\n",
    "- The Backend provides different Managers to create the Audio Objects\n",
    "- and is also responsible for creating the Audio Object EventHandlers that will create sound in the backend \n",
    "\n",
    "\n",
    "- The Audio Objects  are the available building blocks for the sonification which are inspired by common concepts from sound synthesis software\n",
    "- The different objects offer actions like f.e. starting a Synth with a certain frequency\n",
    "These however do not directly generate sounds but rather create Events \n",
    "\n",
    "The events are then passed to the Context which\n",
    "provides the sonification time for the event.\n",
    "The Event then will be inserted into the timeline as a bundles \n",
    "\n",
    "The Timeline then forms a data structure that contains all the actions from the sonification\n",
    "\n",
    "To actually listen to the sonification the events in the timeline can be rendered offline or by the playback\n",
    "\n",
    "The playback offers an interactive control over the sonification and allows f.e. filtering by data source via the BundleProcessor\n",
    "before passing the events to the Audio Object EventHandler in the Backend \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b56460",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0ce605",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcee152",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(context.synths.buffer_synthdefs[\"playbuf\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343df2de",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "context.timeline.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4b9184",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
