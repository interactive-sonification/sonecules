{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sonecules - An Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sonecules have been introduced in our ICAD 2023 paper [Sonecules - A Python Sonification Architecture](https://pub.uni-bielefeld.de/record/2979095) as aggregates that entangle data (constraints and processing), transformation (e.g. mapping), sound synthesis and interaction. \n",
    "\n",
    "Under the umbrella of sonification (as any technique that creates data-driven sound in a systematic, reproducible and objective way) there are a number of paradigms how data and sound relate, such as \n",
    "- Earcons: the data are messages conveyed by using abstract (mostly musical) patterns (or motifs)\n",
    "- Auditory Icons: the data are messages conveyed by selected sounds that metaphorically associate the message\n",
    "- Audification: the data become the sound wave\n",
    "- Parameter Mapping Sonification: the data become the score/instructions performed upon an instrument (aka synth)\n",
    "- Model-Based Sonification: the data become a virtual acoustic object (aka instrument), yet playing is left to the user\n",
    "- Wave-Space Sonification: the data become a process that navigates a sound space (i.e. wave space)\n",
    "\n",
    "These constitute Sonification Techniques - and in fact are more families of methods than a single method. Specifically each technique offers a number of **sonification designs** which are more specific, and require specific constraints on the data (e.g. to be a time series, or to be continuous valued, etc.) or the sound (e.g. to be a continuous stream or to be a collection of similarly structured sound events, etc.)\n",
    "\n",
    "With **Sonecules** we refer to these designs, which, however specific, still allow a multitude of adaptations and refinements to be useful for a specific data set from a specific application and a specific listening context. In our paper we argue\n",
    "\n",
    "> Sonifications are never monolithic, they are composites, consisting of data, a transformation (e.g. mapping or model), interaction (optionally), control parameters and their user interface, but most importantly sound computation. Inspired by chemistry where the properties of substances depend on their specific aggregation (chemical bonds) and feature manifold characteristics depending on their interconnection, we invented the word sonecules (reminding of molecules) to echo this fact. This ties nicely into the use of our mesonic framework - by which we mediate from high-level control to the sound computing side. The name, however, contains the elementary particle ’meson’, which indeed points at a level of finer resolution. An accidental coincidence is that the primarily featured sound computing system is SuperCollider - the name carrying the association of a particle accelerator, an apparatus to study the smallest existing particles. Perhaps a single sample of a sound signal might be that ultimate elementary particle of sound and be the ultimately lowest level of sound, but let’s turn back to the sonecules level.\n",
    ">\n",
    "> The focus of this paper is the sonecule, a single coherent sonification unit. A sonification design entails a concrete usage as in an object-oriented interface. As a good example for a sonecule we can take any specific Model-Based Sonification Design (aka Sonification Model [42]) such as the Data Sonogram Sonifiation model, because it is very clearly described how such a model is constructed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial set of sonecule classes provided are named according to a speaking name part which explains a part of the idea or implementation and a three letter suffix to anchor the sonecule in the realm of paradigms / technigues\n",
    "\n",
    "- **`PMS`**: for Parameter Mapping Sonification \n",
    "- **`AUD`**: for Audification\n",
    "- **`MBS`**: for Model-based Sonification\n",
    "- **`EAR`**: for Earcons\n",
    "- **`PAI`**: for Parameterized Auditory Icons\n",
    "- **`WSS`**: for Wave Space Sonification\n",
    "\n",
    "Under each of these we anticipate the number of soneceules to grow into a potentially larger set so that finally users of the Soneceules package can easily and reliably select, use, refine, and study a set of hundreds of methods.\n",
    "\n",
    "As sonecules is Open Source it allows full introspection in the inner workings of the methods.\n",
    "\n",
    "We embrace contributions and like to integrate novel sonecules into the available library.\n",
    "\n",
    "We will also (in future) think of a package manager/loading mechanism that will allow users to maintain their own private sonecule collection that can be dynamically be included when needed, similar to quarks in SuperCollider.\n",
    "\n",
    "For now, Sonecules is in its early stage, and the main target is to make the community aware of it, to establish it as platform for sonification, and to smooth out the bumps and problems that software naturally has after initial release. So please don't hesitate to come back to us with problems, particularly bugs and feature requests or other ideas. Please contact us via [mail (thermann@uni-bielefeld.de and dreinsch@techfak.de)](mailto:thermann@techfak.uni-bielefeld.de,%20dreinsch@techfak.de) or use the [GitHub issue tracker](https://github.com/interactive-sonification/sonecules). We hope that within some months rapid releases of updates will address the most urgent issues.\n",
    "\n",
    "## Backends\n",
    "\n",
    "Sonecules abstract from the synthesis backend, be it SuperCollider, pya, Chuck, PureData, Csound, Webaudio etc. This abstraction is reached by means of a 'middleware' framework, also of our own making, called [mesonic](https://github.com/interactive-sonification/mesonic). mesonic was designed specifically as Sonification framework which provides backend-independent concepts specific to sonification like the mesonic Synth, Buffer and especially the Timeline. This allows to define sonifications as sonecules and abstract from the backend and allows to introduce new backends as needed and developed.\n",
    "The initial backend during development was SuperCollider3, however, interfaced by means of another package from our toolstack, namely [sc3nb](https://github.com/interactive-sonification/sc3nb). This is all properly wrapped so that end-users don't need to learn nor master the details of these lower layers.\n",
    "\n",
    "As of now (August 2023), SuperCollider is still the predominant (and only fully supported) backend, however, we recently have added [pya](https://github.com/interactive-sonification/pya) (our own python Audio coding Package) as a second backend. It is in nascent state and so far sonecules can't easily switch to that backend - but it is on our roadmap to fully integrate pya, so that many use cases won't require SuperCollider to be installed. Also, rendering directly into pya Asigs (which base on numpy ndarrays) offers the benefit of control beyond the splitted control rate/audio rate separation of many backends, and furthermore allows to work with buffers without any transport between language and backend engine (since they are the same in pya).\n",
    "\n",
    "However, we acknowledge that some engines will be particularly apt for some sonification types. With time we hope to complement supported backends by the favoured ones in the ICAD / AD / Sonification communitites."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Sonecules Library (as of 2023-07-30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audification\n",
    "\n",
    "- [BasicAUD](sonecule-AUD-BasicAUD.ipynb) is a simple Audification of 1D data, i.e. a single data series.\n",
    "\n",
    "- [PhasorAUD](sonecule-AUD-PhasorAUD.ipynb) is a simple looping Audification of 1D data, which allows improved looped playback of a user-selectable data segment\n",
    "\n",
    "These two AUDs are obviously similiar but offers slightly different benefits. This should accentuate that we belief that a sonecule does not need to be the *one* Audification sonecule but that there is room for many different approaches.\n",
    "\n",
    "- [MultivariateBasicAUD](sonecule-AUD-MultivariateBasicAUD.ipynb) is a multi-channel audification that enables to play $n$ channels simultaneously, at this time spatially distributed via the available audio channels, e.g. using stereo panning to route $k$ channels to 2 outputs. It is also an example of a compound sonecule as it is implemented using multiple BasicAUD instances. This highlights the ability to combine different, or in this case the same sonification to create a more complex sonifcation. \n",
    "\n",
    "### Parameter Mapping Sonification\n",
    "\n",
    "- [DiscretePMS](sonecule-PMS-DiscretePMS.ipynb) is a basic Discrete Parameter Mapping Sonification, allowing to map data features to synthesis parameters of a default or self-defined synthesizer, supported by a easy-to-learn and easy-to-modify and store mapping specification (as a dictionary). The mapping is computed feature-by-feature, thus efficient for longer data sets. As trade-off, no mapping rules can be chosen that consider the values of other features (see DiscreteCallbackPMS for these)\n",
    "\n",
    "- [DiscreteCallbackPMS](sonecule-PMS-DiscreteCallbackPMS.ipynb) is a basic Discrete Parameter Mapping Sonification which requires a Python callback function to compute the mapping from data to parameters. Different from DiscretePMS, the callback function receives each complete row vector of a data set and thus can orchestrate the mapping to consider all features.\n",
    "\n",
    "- [ContinuousPMS](sonecule-PMS-ContinuousPMS.ipynb) is a basic continuous Parameter Mapping Sonification, allowing to modulate synthesis parameters of a continuously playing synths while traversing through the data. While this makes usually only sense with data that has a meaningful sorting criterium (e.g. by time), it can be applied to all data sets (but be careful with the interpretation). It computes the set messages for the parameters column-wise, similar to DiscretePMS\n",
    "\n",
    "- [ContinuousCallbackPMS](sonecule-PMS-ContinuousCallBackPMS.ipynb) is a basic continuous Parameter Mapping Sonification which requires a Python callback function to compute the mapping from data to parameters. Different from ContinuousPMS, the callback function receives each complete row vector of a data set and thus can orchestrate the mapping to consider all features.\n",
    "\n",
    "- [TVOscBankPMS](sonecule-PMS-TVOscBankPMS.ipynb) is a special continuous Parameter Mapping Sonification which creates a set of $d$ oscillators tuned to their individual default frequencies, ($d$ refering to the dimension, i.e., nr. of features of the data set). The variations of a feature then map to deviations of these frequencies and values of the amplitudes, according to the selected mode and sonecule arguments. The transport mechanism to the backend is that of individual set messages for each data point, making the method a class in scoreson. For high-frequency data we see utility in a companion sonecule that operates via buffers. Such a sonecules is on the roadmap for a future release.\n",
    "\n",
    "- [TimbralPMS](sonecule-PMS-TimbralPMS.ipynb) is a special continuous parameter mapping sonification which is a special case of TVOscBankPMS: for each channel in the data set, a harmonic (i.e., integer multiple of the fundamental frequency) of an additive synthesis model is set up and controlled by the data. \n",
    "\n",
    "\n",
    "### Model-Based Sonification\n",
    "\n",
    "- [DataSonogramMBS](sonecule-MBS-DataSonogramMBS.ipynb) is a Implementation of the Data Sonogram Sonification Model and provides a visual representation to interact with it. This demonstrates that a sonecule can offer additional interfaces and allows to create interactive sonifications.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Planned in the future\n",
    "\n",
    "a selection of sonecules that are planned to be implemented.\n",
    "\n",
    "### Audification\n",
    "\n",
    "- GranularAUD – using TGrains UGen to navigate audification\n",
    "\n",
    "### Parameter Mapping Sonification\n",
    "\n",
    "- PolyphonicPMS\n",
    "- AuditoryGraphPMS\n",
    "- TVOscBankBufPMS - similar to TVOscBankPMS, but using Buffers for data control\n",
    "\n",
    "### Earcons\n",
    "\n",
    "- BasicMotifEAR\n",
    "\n",
    "### Model-Based Sonification\n",
    "\n",
    "- DataCrystallizationMBS\n",
    "- GrowingNeuralGasMBS\n",
    "- ParticleTrajectoryMBS\n",
    "- PrincipalCurveMBS\n",
    "\n",
    "### Wave Space Sonification\n",
    "\n",
    "- Basic2dWSS\n",
    "- GenericWSS\n",
    "- CanonicalWSS\n",
    "- SampleBasedWSS\n",
    "- DataDrivenLocalizedWSS\n",
    "- GranularWSS\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
